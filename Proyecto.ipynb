{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_location):\n",
    "    \"\"\"\n",
    "    Load data from a CSV file and separate it into features and target variables.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_location : str\n",
    "        The path to the CSV file containing the dataset.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    features : pandas.DataFrame\n",
    "        A DataFrame containing all columns except the first and the last.\n",
    "    \n",
    "    target : pandas.Series\n",
    "        A Series containing the last column of the dataset.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - The function assumes the dataset has at least two columns.\n",
    "    - The first column is ignored, and the last column is treated as the target variable.\n",
    "    - Requires pandas to be imported as `pd`.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(file_location)\n",
    "    features = data.iloc[:,1:-1]\n",
    "    target = data.iloc[:,-1]\n",
    "    return features,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(features,target,size=0.25):\n",
    "    \"\"\"\n",
    "    Split the dataset into training and validation sets.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    features : pandas.DataFrame\n",
    "        The feature variables (independent variables).\n",
    "    \n",
    "    target : pandas.Series\n",
    "        The target variable (dependent variable).\n",
    "    \n",
    "    size : float, optional (default=0.25)\n",
    "        The proportion of the dataset to include in the validation split.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    features_train : pandas.DataFrame\n",
    "        The training set features.\n",
    "    \n",
    "    features_valid : pandas.DataFrame\n",
    "        The validation set features.\n",
    "    \n",
    "    target_train : pandas.Series\n",
    "        The training set target values.\n",
    "    \n",
    "    target_valid : pandas.Series\n",
    "        The validation set target values.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - Uses `train_test_split` from `sklearn.model_selection`, so ensure it is imported.\n",
    "    - `random_state` is used for reproducibility.\n",
    "    - Default validation size is 25% of the dataset.\n",
    "    \"\"\"\n",
    "    features_train, features_valid, target_train, target_valid = train_test_split(features,target,test_size=size,random_state=42)\n",
    "    return features_train, features_valid, target_train, target_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(features_train, features_valid, target_train, target_valid):\n",
    "    \"\"\"\n",
    "    Train a Linear Regression model and evaluate its performance.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    features_train : pandas.DataFrame\n",
    "        The training set features.\n",
    "    \n",
    "    features_valid : pandas.DataFrame\n",
    "        The validation set features.\n",
    "    \n",
    "    target_train : pandas.Series\n",
    "        The training set target values.\n",
    "    \n",
    "    target_valid : pandas.Series\n",
    "        The validation set target values.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    predictions : numpy.ndarray\n",
    "        The predicted values for the validation set.\n",
    "\n",
    "    Prints:\n",
    "    -------\n",
    "    - Mean reserve volume predicted.\n",
    "    - Root Mean Squared Error (RMSE) of the model.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - Uses `LinearRegression` from `sklearn.linear_model`, so ensure it is imported.\n",
    "    - Assumes `root_mean_squared_error` is defined elsewhere or imported.\n",
    "    \"\"\"\n",
    "    model = LinearRegression()\n",
    "    model.fit(features_train,target_train)\n",
    "    predictions = model.predict(features_valid)\n",
    "    print(f\"Mean reserve volume predicted: {predictions.mean()}\")\n",
    "    print(f\"RSME: {root_mean_squared_error(target_valid,predictions)}\")\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(file_location):\n",
    "    \"\"\"\n",
    "    Load data, split it into training and validation sets, train a model, and generate predictions.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_location : str\n",
    "        The path to the CSV file containing the dataset.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    predictions : numpy.ndarray\n",
    "        The predicted values for the validation set.\n",
    "\n",
    "    target_valid : pandas.Series\n",
    "        The actual target values for the validation set.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - Calls `load_data()` to load and preprocess the dataset.\n",
    "    - Uses `np.random.RandomState(42)` to ensure reproducibility.\n",
    "    - Calls `split_data()` to divide the data into training and validation sets.\n",
    "    - Calls `model()` to train a Linear Regression model and generate predictions.\n",
    "    - Assumes that `load_data`, `split_data`, and `model` are defined elsewhere.\n",
    "    - Requires `numpy` to be imported as `np`.\n",
    "    \"\"\"\n",
    "    features, target = load_data(file_location)\n",
    "    features_train, features_valid, target_train, target_valid = split_data(features,target)\n",
    "    predictions = model(features_train, features_valid, target_train, target_valid)\n",
    "    return predictions,target_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reserve volume predicted: 92.39879990657768\n",
      "RSME: 37.75660035026169\n"
     ]
    }
   ],
   "source": [
    "predictions_0,target_valid_0 = process(\"datasets/geo_data_0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reserve volume predicted: 68.71287803913762\n",
      "RSME: 0.8902801001028854\n"
     ]
    }
   ],
   "source": [
    "predictions_1,target_valid_1 = process(\"datasets/geo_data_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reserve volume predicted: 94.77102387765939\n",
      "RSME: 40.145872311342174\n"
     ]
    }
   ],
   "source": [
    "predictions_2,target_valid_2 = process(\"datasets/geo_data_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "investment = 100000000\n",
    "gain_per_unit = 4500\n",
    "n_oil_wells = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average volume of oil per oil well to avoid losses: 111.11\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average volume of oil per oil well to avoid losses: {investment/n_oil_wells/gain_per_unit:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La región 0 tiene en promedio 9 unidades menos de las necesarias para evitar pérdidas. Pero el error es de 37 unidades y se toman en cuenta todos los pozos y no sólo los mejores 200, por lo que hay muy alta posibilidad de que sea una buena región.\n",
    "- La región 1 tiene en promedio 42 unidades menos de las necesarias para evitar pérdidas. Aunque se tomen a los mejores 200 pozos, el error es mínimo en esta región y el déficit es bastante grande, por lo que tal vez no sea tan buena idea invertir aquí.\n",
    "- La región 2 es muy similar a la 0, con un poco más de unidades en promedio pero con un poco más de error. Ambas regiones, 0 y 2, son potencialmente buenas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_ganancia(predictions):\n",
    "    investment = 100000000\n",
    "    gain_per_unit = 4500\n",
    "    predictions_top_200 = pd.Series(predictions).sort_values(ascending=False)[:200]\n",
    "    total_reserves_predicted = predictions_top_200.sum()\n",
    "    benefit = total_reserves_predicted*gain_per_unit - investment\n",
    "    print(f\"Benefit: {benefit:.2f}\")\n",
    "    return benefit, predictions_top_200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benefit: 38966584.80\n"
     ]
    }
   ],
   "source": [
    "benefit_0, predictions_top_200_0 = calcular_ganancia(predictions_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benefit: 24869381.15\n"
     ]
    }
   ],
   "source": [
    "benefit_1, predictions_top_200_1 = calcular_ganancia(predictions_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benefit: 33779815.14\n"
     ]
    }
   ],
   "source": [
    "benefit_2, predictions_top_200_2 = calcular_ganancia(predictions_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En las 3 regiones se obtiene una buena cantidad de beneficio. Pero se confirman mis sospechas de que la región 0 y la región 2 son potencialmente mucho mejor que la región 1. Aquí vemos que la región 0 potencialmente es la mejor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculo_riesgos_y_ganancias(predictions_top_200):\n",
    "    investment = 100000000\n",
    "    gain_per_unit = 4500\n",
    "    n_oil_wells = 200\n",
    "    state = np.random.RandomState(42)\n",
    "    means = []\n",
    "    for i in range(1000):\n",
    "        means.append(predictions_top_200.sample(frac=1,replace=True,random_state=state).mean())\n",
    "    means = pd.Series(means)\n",
    "    mean = means.mean()\n",
    "    lower = means.quantile(q=0.025) #Intervalo de confianza de las medias de volumen de petroleo\n",
    "    upper = means.quantile(q=0.975) #Intervalo de confianza\n",
    "    benefit_mean = mean*n_oil_wells*gain_per_unit - investment\n",
    "    benefit_lower = lower*n_oil_wells*gain_per_unit - investment\n",
    "    benefit_upper = upper*n_oil_wells*gain_per_unit - investment\n",
    "    benefit_interval = np.array((benefit_lower,benefit_upper)) #Intervalo de confianza del beneficio\n",
    "    n_losses = (means<=111.11).sum() #Número de casos en donde el promedio de volumen es menor al requerido para evitar perdidas\n",
    "    loss_probability = n_losses/1000\n",
    "    print(f\"Beneficio promedio: {benefit_mean}\\nIntervalo de confianza del 95%: {benefit_interval}\\nProbabilidad de pérdida: {loss_probability*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beneficio promedio: 38974073.74300763\n",
      "Intervalo de confianza del 95%: [38193516.72505969 39814410.77221969]\n",
      "Probabilidad de pérdida: 0.0%\n"
     ]
    }
   ],
   "source": [
    "calculo_riesgos_y_ganancias(predictions_top_200_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beneficio promedio: 24869674.02385436\n",
      "Intervalo de confianza del 95%: [24836070.83547185 24905925.30863419]\n",
      "Probabilidad de pérdida: 0.0%\n"
     ]
    }
   ],
   "source": [
    "calculo_riesgos_y_ganancias(predictions_top_200_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beneficio promedio: 33785431.59368783\n",
      "Intervalo de confianza del 95%: [33132193.03085911 34475846.62835312]\n",
      "Probabilidad de pérdida: 0.0%\n"
     ]
    }
   ],
   "source": [
    "calculo_riesgos_y_ganancias(predictions_top_200_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí se confirma lo que había visto arriba. Ninguna región tiene riezgo de pérdida, pero la que mejores ganancias da es la región 0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
