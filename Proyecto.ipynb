{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_location):\n",
    "    \"\"\"\n",
    "    Load data from a CSV file and separate it into features and target variables.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_location : str\n",
    "        The path to the CSV file containing the dataset.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    features : pandas.DataFrame\n",
    "        A DataFrame containing all columns except the first and the last.\n",
    "    \n",
    "    target : pandas.Series\n",
    "        A Series containing the last column of the dataset.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - The function assumes the dataset has at least two columns.\n",
    "    - The first column is ignored, and the last column is treated as the target variable.\n",
    "    - Requires pandas to be imported as `pd`.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(file_location)\n",
    "    features = data.iloc[:,1:-1]\n",
    "    target = data.iloc[:,-1]\n",
    "    return features,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(features,target,state,size=0.25):\n",
    "    \"\"\"\n",
    "    Split the dataset into training and validation sets.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    features : pandas.DataFrame\n",
    "        The feature variables (independent variables).\n",
    "    \n",
    "    target : pandas.Series\n",
    "        The target variable (dependent variable).\n",
    "    \n",
    "    state : int\n",
    "        The random seed for reproducibility.\n",
    "    \n",
    "    size : float, optional (default=0.25)\n",
    "        The proportion of the dataset to include in the validation split.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    features_train : pandas.DataFrame\n",
    "        The training set features.\n",
    "    \n",
    "    features_valid : pandas.DataFrame\n",
    "        The validation set features.\n",
    "    \n",
    "    target_train : pandas.Series\n",
    "        The training set target values.\n",
    "    \n",
    "    target_valid : pandas.Series\n",
    "        The validation set target values.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - Uses `train_test_split` from `sklearn.model_selection`, so ensure it is imported.\n",
    "    - `random_state` is used for reproducibility.\n",
    "    - Default validation size is 25% of the dataset.\n",
    "    \"\"\"\n",
    "    features_train, features_valid, target_train, target_valid = train_test_split(features,target,test_size=size,random_state=state)\n",
    "    return features_train, features_valid, target_train, target_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(features_train, features_valid, target_train, target_valid):\n",
    "    \"\"\"\n",
    "    Train a Linear Regression model and evaluate its performance.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    features_train : pandas.DataFrame\n",
    "        The training set features.\n",
    "    \n",
    "    features_valid : pandas.DataFrame\n",
    "        The validation set features.\n",
    "    \n",
    "    target_train : pandas.Series\n",
    "        The training set target values.\n",
    "    \n",
    "    target_valid : pandas.Series\n",
    "        The validation set target values.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    predictions : numpy.ndarray\n",
    "        The predicted values for the validation set.\n",
    "\n",
    "    Prints:\n",
    "    -------\n",
    "    - Mean reserve volume predicted.\n",
    "    - Root Mean Squared Error (RMSE) of the model.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - Uses `LinearRegression` from `sklearn.linear_model`, so ensure it is imported.\n",
    "    - Assumes `root_mean_squared_error` is defined elsewhere or imported.\n",
    "    \"\"\"\n",
    "    model = LinearRegression()\n",
    "    model.fit(features_train,target_train)\n",
    "    predictions = model.predict(features_valid)\n",
    "    print(f\"Mean reserve volume predicted: {predictions.mean()}\")\n",
    "    print(f\"RSME: {root_mean_squared_error(target_valid,predictions)}\")\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(file_location):\n",
    "    \"\"\"\n",
    "    Load data, split it into training and validation sets, train a model, and generate predictions.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_location : str\n",
    "        The path to the CSV file containing the dataset.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    predictions : numpy.ndarray\n",
    "        The predicted values for the validation set.\n",
    "\n",
    "    target_valid : pandas.Series\n",
    "        The actual target values for the validation set.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - Calls `load_data()` to load and preprocess the dataset.\n",
    "    - Uses `np.random.RandomState(42)` to ensure reproducibility.\n",
    "    - Calls `split_data()` to divide the data into training and validation sets.\n",
    "    - Calls `model()` to train a Linear Regression model and generate predictions.\n",
    "    - Assumes that `load_data`, `split_data`, and `model` are defined elsewhere.\n",
    "    - Requires `numpy` to be imported as `np`.\n",
    "    \"\"\"\n",
    "    features, target = load_data(file_location)\n",
    "    state = np.random.RandomState(42)\n",
    "    features_train, features_valid, target_train, target_valid = split_data(features,target,state)\n",
    "    predictions = model(features_train, features_valid, target_train, target_valid)\n",
    "    return predictions,target_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reserve volume predicted: 92.39879990657768\n",
      "RSME: 37.75660035026169\n"
     ]
    }
   ],
   "source": [
    "predictions_0,target_valid_0 = process(\"datasets/geo_data_0.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reserve volume predicted: 68.71287803913762\n",
      "RSME: 0.8902801001028854\n"
     ]
    }
   ],
   "source": [
    "predictions_1,target_valid_1 = process(\"datasets/geo_data_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reserve volume predicted: 94.77102387765939\n",
      "RSME: 40.145872311342174\n"
     ]
    }
   ],
   "source": [
    "predictions_2,target_valid_2 = process(\"datasets/geo_data_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "investment = 100000000\n",
    "gain_per_unit = 4500\n",
    "n_oil_wells = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average volume of oil per oil well to avoid losses: 111.11\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average volume of oil per oil well to avoid losses: {investment/n_oil_wells/gain_per_unit:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La región 0 tiene en promedio 9 unidades menos de las necesarias para evitar pérdidas. Pero el error es de 37 unidades y se toman en cuenta todos los pozos y no sólo los mejores 200, por lo que hay muy alta posibilidad de que sea una buena región.\n",
    "- La región 1 tiene en promedio 42 unidades menos de las necesarias para evitar pérdidas. Aunque se tomen a los mejores 200 pozos, el error es mínimo en esta región y el déficit es bastante grande, por lo que tal vez no sea tan buena idea invertir aquí.\n",
    "- La región 2 es muy similar a la 0, con un poco más de unidades en promedio pero con un poco más de error. Ambas regiones, 0 y 2, son potencialmente buenas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
